{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Brief: Fundamentals of Numpy and Pandas for Machine Learning  \n",
    "\n",
    "## Deadline: 03 November 2023, 14:00 GMT\n",
    "\n",
    "## Number of marks available: 10\n",
    "\n",
    "In this practical, we will practice using numpy and pandas to implement the fundamentals of machine learning experiments such as data splitting, imputation, and model training and evaluation. \n",
    "\n",
    "### Please READ the whole assignment first, before starting to work on it.\n",
    "\n",
    "### How and what to submit\n",
    "\n",
    "A. A **Jupyter Notebook** with the code in all the cells executed and outputs displayed.\n",
    "\n",
    "B. Name your Notebook **COMP61011_AssignmentA1_XXXXXX.ipynb** where XXXXXX is your username such as such as abc18de. Example: `COMP61011_AssignmentA1_abc18de.ipynb`\n",
    "\n",
    "C. Upload the Jupyter Notebook in B to Blackboard under the **Computing Assignment (numpy and pandas)** submission area before the deadline. **There are two submissions: please pay close attention to submit to the right place!**\n",
    "\n",
    "D. **NO DATA UPLOAD**: Please do not upload the data files used in this Notebook. We have a copy already. \n",
    "\n",
    "\n",
    "### Assessment Criteria \n",
    "\n",
    "* Being able to use numpy and pandas to preprocess a dataset.\n",
    "\n",
    "* Being able to follow the steps involved in an end-to-end project in machine learning.\n",
    "\n",
    "* Be able to implement, from scratch, a linear model and train it using gradient descent.\n",
    "\n",
    "\n",
    "### Code quality and use of Python libraries\n",
    "When writing your code, you will find out that there are operations that are repeated at least twice. These operations should be carried out in functions. Furthermore, if your code is unreadable, we may not award marks for that section. Make sure to check the following:\n",
    "\n",
    "* Did you include Python functions to solve the question and avoid repeating code? \n",
    "* Did you comment your code to make it readable to others?\n",
    "\n",
    "**DO NOT USE scikit-learn for the questions on this assignment. You are meant to write Python code from scratch. Using scikit-learn for the questions on this assignment will give ZERO marks. No excuse will be accepted.**\n",
    "\n",
    "Furthermore, please try to avoid using any imports apart from the ones already provided in the Notebook. You can easily install all recommended modules for this assignment by running the following command in your terminal: `python -m pip install -r requirements.txt`\n",
    "\n",
    "\n",
    "### Late submissions\n",
    "\n",
    "We follow Department's guidelines about late submissions, i.e., a deduction of 10% of the mark each 24 hours the work is late after the deadline. NO late submission will be marked one week after the deadline. Please read [this link](https://wiki.cs.manchester.ac.uk/index.php/UGHandbook23:Main#Late_Submission_of_Coursework_Penalty). \n",
    "\n",
    "### Use of unfair means \n",
    "\n",
    "**Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.** Please carefully read [what constitutes Unfair Means](https://documents.manchester.ac.uk/display.aspx?DocID=2870) if not sure. If you still have questions, please ask your Personal tutor or the Lecturers.\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: regularised ridge regression and gradient descent\n",
    "\n",
    "Regularisation is a technique commonly used in Machine Learning to prevent overfitting. It consists on adding terms to the objective function such that the optimisation procedure avoids solutions that just learn the training data. Popular techniques for regularisation in Supervised Learning include [Lasso Regression](https://en.wikipedia.org/wiki/Lasso_(statistics)), [Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) and the [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization). \n",
    "\n",
    "Here we will build a Ridge Regression model, and implement equations to optimise the objective function using the update rules for gradient descent. You will use those update rules for making predictions on the Wine Quality dataset.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "Let us start with a data set for training $\\mathcal{D} = \\{\\mathbf{y}, \\mathbf{X}\\}$, where the vector $\\mathbf{y}=[y_1, \\cdots, y_N]^{\\top}$ and $\\mathbf{X}$ is the design matrix from Lab 3, this is, \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} = \n",
    "                \\begin{bmatrix}\n",
    "                        1 & x_{1,1} & \\cdots & x_{1, D}\\\\\n",
    "                        1 & x_{2,1} & \\cdots & x_{2, D}\\\\\n",
    "                   \\vdots &  \\vdots\\\\\n",
    "                        1 & x_{N,1} & \\cdots & x_{N, D}\n",
    "                \\end{bmatrix}\n",
    "               = \n",
    "               \\begin{bmatrix}\n",
    "                      \\mathbf{x}_1^{\\top}\\\\\n",
    "                       \\mathbf{x}_2^{\\top}\\\\\n",
    "                          \\vdots\\\\\n",
    "                        \\mathbf{x}_N^{\\top}\n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Our predictive model is going to be a linear model\n",
    "\n",
    "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "\n",
    "where $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$.\n",
    "\n",
    "The **objective function** we are going to use has the following form\n",
    "\n",
    "$$ E(\\mathbf{w}, \\lambda) = \\frac{1}{N}\\sum_{n=1}^N (y_n - f(\\mathbf{x}_n))^2 + \\frac{\\lambda}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "where $\\lambda>0$ is known as the *regularisation* parameter.\n",
    "\n",
    "This objective function was studied in Lecture 3. \n",
    "\n",
    "The first term on the rhs is what we call the \"fitting\" term whereas the second term in the expression is the regularisation term. Given $\\lambda$, the two terms in the expression have different purposes. The first term is looking for a value of $\\mathbf{w}$ that leads the squared-errors to zero. While doing this, $\\mathbf{w}$ can take any value and lead to a solution that it is only good for the training data but perhaps not for the test data. The second term is regularising the behavior of the first term by driving the $\\mathbf{w}$ towards zero. By doing this, it restricts the possible set of values that $\\mathbf{w}$ might take according to the first term. The value that we use for $\\lambda$ will allow a compromise between a value of $\\mathbf{w}$ that exactly fits the data (first term) or a value of $\\mathbf{w}$ that does not grow too much (second term).\n",
    "\n",
    "This type of regularisation has different names: ridge regression, Tikhonov regularisation or $\\ell_2$ norm regularisation. \n",
    "\n",
    "### Optimising the objective function with respect to $\\mathbf{w}$\n",
    "\n",
    "There are two ways we can optimise the objective function with respect to $\\mathbf{w}$. The first one leads to a closed form expression for $\\mathbf{w}$ and the second one using an iterative optimisation procedure that updates the value of $\\mathbf{w}$ at each iteration by using the gradient of the objective function with respect to $\\mathbf{w}$,\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}},\n",
    "$$\n",
    "where $\\eta$ is the *learning rate* parameter and $\\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}}$ is the gradient of the objective function.\n",
    "\n",
    "It can be shown (this is a question in the Exercise Sheet 3) that a closed-form expression for the optimal $\\mathbf{w}_*$ is given as\n",
    "\n",
    "\\begin{align*}            \n",
    "            \\mathbf{w}_*& = \\left(\\mathbf{X}^{\\top}\\mathbf{X} + \\frac{\\lambda N}   \n",
    "                                     {2}\\mathbf{I}\\right)^{-1}\\mathbf{X}^{\\top}\\mathbf{y}.\n",
    "\\end{align*}\n",
    "\n",
    "Alternatively, we can find an update equation for $\\mathbf{w}_{\\text{new}}$ using gradient descent leading to:\n",
    "\n",
    "\\begin{align*}\n",
    "   \\mathbf{w}_{\\text{new}} & = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}\n",
    "                              {d\\mathbf{w}},\\\\\n",
    "                           & = \\mathbf{w}_{\\text{old}} +  \\frac{2\\eta}{N}\\sum_{n=1}^N   \n",
    "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n  \n",
    "                       - \\eta\\lambda\\mathbf{w}_{\\text{old}}\\\\\n",
    "                           & = (1 - \\eta\\lambda)\\mathbf{w}_{\\text{old}} + \\frac{2\\eta}\n",
    "                               {N}\\sum_{n=1}^N   \n",
    "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-set up: imports and random seed\n",
    "\n",
    "**Important: set a random seed below that corresponds to the last five digits of your student ID number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(xxxxx) # replace xxxxx with the last 5 digits of your student ID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Wine Quality\n",
    "\n",
    "The dataset you will use in this assignment comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). **We are going to use ridge regression for predicting wine quality**. The description of the dataset can be found [on the UCI website](https://archive.ics.uci.edu/dataset/186/wine+quality) and in the extracted file `winequality.names`. A copy of the dataset is provided in the file `winequality-white-missing.csv`.\n",
    "\n",
    "We can view some of the rows in the dataset with the `.sample()` method, or print the first few rows with the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.038</td>\n",
       "      <td>73.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.98900</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.25</td>\n",
       "      <td>12.9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.36</td>\n",
       "      <td>13.10</td>\n",
       "      <td>0.050</td>\n",
       "      <td>35.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>8.9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.062</td>\n",
       "      <td>31.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.99170</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.44</td>\n",
       "      <td>11.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.32</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.031</td>\n",
       "      <td>41.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.99234</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.26</td>\n",
       "      <td>12.3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.23</td>\n",
       "      <td>17.30</td>\n",
       "      <td>0.047</td>\n",
       "      <td>37.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99906</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.46</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.48</td>\n",
       "      <td>19.45</td>\n",
       "      <td>0.048</td>\n",
       "      <td>55.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.31</td>\n",
       "      <td>13.90</td>\n",
       "      <td>0.040</td>\n",
       "      <td>57.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.99672</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>10.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99440</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>7.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.32</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.056</td>\n",
       "      <td>39.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.99393</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.52</td>\n",
       "      <td>10.2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.055</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.99460</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "1126            6.5             0.115         0.29            1.95      0.038   \n",
       "1398            7.3               NaN         0.36           13.10      0.050   \n",
       "50              7.2             0.190         0.31            1.60      0.062   \n",
       "4737            6.1             0.240         0.32            9.00      0.031   \n",
       "4421            6.6             0.220         0.23           17.30      0.047   \n",
       "1043            7.5             0.330         0.48           19.45      0.048   \n",
       "2876            6.4             0.220         0.31           13.90      0.040   \n",
       "2165            7.0             0.350         0.31            1.80      0.069   \n",
       "3485            7.5               NaN         0.32            4.80      0.056   \n",
       "1567            7.5             0.190         0.49            1.80      0.055   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "1126                 73.0                 166.0  0.98900  3.12       0.25   \n",
       "1398                 35.0                 200.0  0.99860  3.04       0.46   \n",
       "50                   31.0                 173.0  0.99170  3.35       0.44   \n",
       "4737                 41.0                 134.0  0.99234  3.25       0.26   \n",
       "4421                 37.0                 118.0  0.99906  3.08       0.46   \n",
       "1043                 55.0                 243.0  1.00100  2.95       0.40   \n",
       "2876                 57.0                 135.0  0.99672  3.21       0.38   \n",
       "2165                 15.0                   NaN  0.99440  3.18       0.47   \n",
       "3485                 39.0                 113.0  0.99393  3.11       0.52   \n",
       "1567                 19.0                 110.0  0.99460  3.33       0.44   \n",
       "\n",
       "      alcohol  quality  \n",
       "1126     12.9        7  \n",
       "1398      8.9        7  \n",
       "50       11.7        6  \n",
       "4737     12.3        7  \n",
       "4421      8.8        6  \n",
       "1043      8.8        5  \n",
       "2876     10.7        5  \n",
       "2165      9.4        5  \n",
       "3485     10.2        7  \n",
       "1567      9.9        5  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data into a pandas dataframe\n",
    "white_wine_data = pd.read_csv('./winequality-white-missing.csv', sep=';', index_col=0)\n",
    "\n",
    "# View 10 random rows of the data\n",
    "white_wine_data.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is the quality score of the wine, which is a number between 0 and 10. The other 11 columns are variables, such as the pH value, the alcohol content, etc. which together make up the *feature vector* of each wine. We will use these feature vectors to predict the quality score of the wine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset (1 mark)\n",
    "\n",
    "Before designing any machine learning model, we need to set aside the test data. We will use the remaining training data for fitting the model. *It is important to remember that the test data has to be set aside before preprocessing*. \n",
    "\n",
    "Any preprocessing that you do has to be calibrated *only* on the training data, and several key statistics from this preprocessing need to be saved for the test stage. Separating the dataset into training and test before any preprocessing has happened helps us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
    "\n",
    "Furthermore, we are going to use *hold-out validation* for validating our predictive model, so we need to further separate the training data into a training set and a validation set.\n",
    "\n",
    "In this step, you should first **shuffle the data**, then split the dataset into a training set, a validation set and a test set: \n",
    "- The training set should have 70% of the total observations,\n",
    "- The validation set should have 15% of the total observations,\n",
    "- The test set should have the remaining 15%. \n",
    "\n",
    "If you have correctly initialised the `default_rng()` object with a random seed above, then you can use `rng.shuffle()` or `rng.random()` to ensure you have a random selection of the data in training and valdation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration (1 mark)\n",
    "Create a copy of the training data for exploration.\n",
    "\n",
    "Have a closer look at the dataset. You can use the `.describe()` method to get some basic statistics about each variable. Create a few plots (e.g., histograms, scatter plots) to visualise the data. What can you say about the features (their distributions, correlations, etc.)?\n",
    "\n",
    "Remember that all graphs should have axes and a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing, dealing with missing values (1 mark)\n",
    "Often, when dealing with real-world data, we will find that some values are missing or nonsensical. We need to deal with these missing values before doing any further analysis. Furthermore, once we have dealt with the missing values, we may want to perform standardisation or normalisation on the training data.\n",
    "- In this dataset, missing values have been filled with `NaN` (Not a Number) values. For features with missing values, use the mean value of the non-missing values of that feature to perform imputation. Save these mean values, you will need them later when performing the validation and testing steps.\n",
    "\n",
    "- Once you have imputed the missing data, apply feature scaling using the given function `get_stats_normalisation()`. This function takes the imputed dataset and normalises all values to be within the range [0, 1], based on physically plausible minimum and maximum values for each feature (saved in `maxs` and `mins`). Since most of these values are not normally distributed, we choose not to standardise the data, but we can make use of the minimum and maximum values of each feature to normalise the data - in this case we can rely on knowledge of physical variables (domain knowledge), as well as observations from the testing data, to set these values. You may use the given `maxs` and `mins` values throughout the rest of the notebook.\n",
    "\n",
    "Make sure to leave brief comments in your code to explain the preprocessing steps you have taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given helper function\n",
    "# Normalise the data - scales between [0,1], use in case features are not normally distributed\n",
    "maxs = np.array([15, 1.1, 1.66, 66, 0.35, 290, 440, 1.05, 3.85, 1.1, 15])\n",
    "mins = np.array([3.8, 0.08, 0.0, 0.5, 0.0, 2.0, 9.0, 0.9, 2.7, 0.2, 8.0])\n",
    "def get_stats_normalisation(data_local, maxs=maxs, mins=mins):\n",
    "    \"\"\"\n",
    "    Takes an input dataframe, computes the max and min values per column, then scales each column\n",
    "    to be within those values. If max and min values are given, scales each column to be within given\n",
    "    range. It assumes the target feature is the last column.\n",
    "\n",
    "    Returns a numpy array of the scaled features. Note that targets are not returned.\n",
    "    \"\"\"\n",
    "    uX = (data_local.iloc[:, :-1]).values # We get the values of the features\n",
    "    N2, D = np.shape(uX)\n",
    "    if maxs is None:        # If max and min values are not given, compute them\n",
    "        maxs = np.amax(uX, 0)   # We get the max values for each column \n",
    "    if mins is None:\n",
    "        mins = np.amin(uX, 0)   # We get the min values for each column\n",
    "    nX = np.empty((N2, D))\n",
    "    for i in range(D):      # scale between min and max values of each feature\n",
    "        nX[:, i] = (uX[:, i] - mins[i])/(maxs[i] - mins[i])  \n",
    "    return nX, maxs, mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training a predictive model (4 marks)\n",
    "\n",
    "We have now curated our training data by removing data observations and features with a large amount of missing values. We have also normalised the feature vectors. We are now in a good position to work on developing the prediction model and validating it. We will build a regularised ridge regression model and train it using gradient descent for iterative optimisation. \n",
    "\n",
    "We first organise the dataframe into the vector of targets $\\mathbf{y}$, call it `yTrain`, and the design matrix $\\mathbf{X}$, call it `XTrain`. Remember to augment `XTrain` with a column of ones: this is the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = ... # The training target labels\n",
    "XTrain = ... # The standardised inputs with an additional column vector  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal $\\mathbf{w}$ with stochastic gradient descent\n",
    "\n",
    "Use gradient descent to iteratively compute the value of $\\mathbf{w}_{\\text{new}}$. Instead of using all the training set in `XTrain` and `yTrain` to compute the gradient, use a subset of $S$ instances in `XTrain` and `yTrain`. This is sometimes called *minibatch gradient descent,* where $S$ is the size of the minibatch. \n",
    "\n",
    "You will need to find the best values for three parameters: $\\eta$ (eta), the learning rate, $S$, the number of datapoints in the minibatch, and $\\gamma$ (gamma), the regularisation parameter. We can do this using a grid search over the validation set. \n",
    "\n",
    "* In this question we will use the validation data. So before proceeding to the next steps, make sure that you replace the missing values in each feature vector with the mean values you computed from the training data, and apply the same feature scaling procedure that you used for the training data. (**1 mark**)\n",
    "\n",
    "* Write a function, `mgd_optimiser`, that takes as input the training data and targets, the learning rate $\\eta$, the minibatch size $S$, the regularisation parameter $\\gamma$, and the number of iterations $T$. The function should return the optimal $\\mathbf{w}$ after the chosen number of iterations. (**1 mark**)\n",
    "    \n",
    "* Create a grid of values for the parameters $\\gamma$ and $\\eta$ using `np.logspace` and a grid of values for $S$ using `np.linspace`. Because you need to find three parameters, start with five values for each parameter in the grid and see if you can increase it (it may take some time to run). Make sure you understand what is the meaning of `np.logspace` and `np.linspace`. Notice that you can use negative values for `start` in `np.logspace`. (**1 mark**)\n",
    "\n",
    "* For each value that you have of $\\gamma$, $\\eta$ and $S$ in your grid, use the training set to compute $\\mathbf{w}$ using your `mgd_optimiser` function, and then measure the RMSE using that $\\mathbf{w}$ over the validation data. For the validation data, make sure you preprocess it before applying the prediction model over it. For the minibatch gradient descent choose to stop the iterative procedure after $200$ iterations. (**1 mark**)\n",
    "\n",
    "* Choose the values of $\\gamma$, $\\eta$ and $S$ that lead to the lowest RMSE and save them. You will use them at the test stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS THE VALIDATION DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE GRID OF VALUES FOR GAMMA, ETA AND S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE REGULARISED LINEAR MODEL AND COMPUTE THE RMSE FOR ALL VALUES OF GAMMA, ETA AND S\n",
    "\n",
    "def mgd_optimiser(X, y, gamma, eta, S, max_iters):\n",
    "    ...\n",
    "    return w\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and results reporting (3 marks)\n",
    "\n",
    "We now know the best model, according to the validation data. We will now put together the training data and the validation data and perform the preprocessing as before, this is, impute the missing values and scale the inputs. We will train the model again using the minibatch stochastic gradient descent and finally compute the RMSE over the test data.\n",
    "\n",
    "* (1) In this question we will use the test data. First, combine the original training and validation data and perform the preprocessing again on this new data. Save the values from this preprocessing step. \n",
    "\n",
    "* Re-train your model on the full training set, using the optimal values of $\\gamma$, $\\eta$ and $S$. (**1 mark**)\n",
    "\n",
    "* Preprocess the test data using the values saved in step (1). (**1 mark**)\n",
    "\n",
    "* Run your model on the test data using the optimal values of of $\\gamma$, $\\eta$ and $S$, and report the RMSE. (**1 mark**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
